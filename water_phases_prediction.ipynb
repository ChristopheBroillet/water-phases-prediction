{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45b970b1",
   "metadata": {},
   "source": [
    "# Mini-project: Prediction of Water Phases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98532384",
   "metadata": {},
   "source": [
    "The goal of this mini project is to compare the performance of different classifiers, in the task of predicting the phase of the water given its temperature and pressure. This project was done during the Automn Semester 2022 at the University of Fribourg."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fbf5a5",
   "metadata": {},
   "source": [
    "---\n",
    "## Libraries\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70d260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = (10, 7.5)\n",
    "plt.rcParams['font.size'] = 15\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "import copy\n",
    "import torch\n",
    "from torch import nn, tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "output_path = Path('output')\n",
    "output_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08679302",
   "metadata": {},
   "source": [
    "---\n",
    "## Loading and handling the data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f79206",
   "metadata": {},
   "source": [
    "For the phase, the label 0, 1 and 2 correspond respectively to gas, liquid and solid. In this project, we will focus only on a temperature range $[200, 575]$ in Kelvin and a pressure range $[10^{-1},10^7]$ in Pascal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf95700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and create a new column 'phase'\n",
    "data = pd.read_csv(\"water_phases_sampling.csv\")\n",
    "label_to_phase = {0: \"Gas\", 1: \"Liquid\", 2: \"Solid\"}\n",
    "data['phase'] = np.array([label_to_phase[label] for label in data['label']])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2978e0",
   "metadata": {},
   "source": [
    "Filter the data to be sure it is in the given range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98c0ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the temperature and the pressure w.r.t. the given range\n",
    "filter_temperature = data['temperature'].between(200, 575, inclusive=\"both\")\n",
    "data = data[filter_temperature]\n",
    "filter_pressure = data['pressure'].between(1e-1, 1e7, inclusive=\"both\")\n",
    "data = data[filter_pressure]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b280acd",
   "metadata": {},
   "source": [
    "### Proportions of the classes in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24da44dd",
   "metadata": {},
   "source": [
    "To have an idea of the proportions in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e928d38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prop_classes(labels):\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    print(f\"\"\"This data has\n",
    "{counts[0]/len(labels)} gas,\n",
    "{counts[1]/len(labels)} liquid and\n",
    "{counts[2]/len(labels)} solid samples (in proportions).\"\"\")\n",
    "\n",
    "prop_classes(data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37874469",
   "metadata": {},
   "source": [
    "First issue here: class imbalance problem, we will need to reduce the number of samples of the *gas* class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3401622f",
   "metadata": {},
   "source": [
    "### Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4d6521",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=data, x='temperature', y='pressure', hue='phase', s=15)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8c3dea",
   "metadata": {},
   "source": [
    "This does not look good, try a logarithmic scale on the *pressure* axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea1edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.yscale('log')\n",
    "sns.scatterplot(data=data, x='temperature', y='pressure', hue='phase', s=15)\n",
    "plt.legend(loc='right')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d9acd0",
   "metadata": {},
   "source": [
    "Better. We will then need to preprocess the data of the *pressure* feature, by applying a *log10* transformation, so that we get a *linear scale* data on both temperature and pressure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3a265a",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff63ee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a log10 function to the pressure feature\n",
    "data['pressure'] = np.log10(data['pressure'])\n",
    "# Class imbalance problem, remove 80% of the gas samples and 10% of the liquid samples\n",
    "balanced_data = data.drop(data[data['phase'] == 'Gas'].sample(frac=0.8).index)\n",
    "balanced_data = balanced_data.drop(balanced_data[balanced_data['phase'] == 'Liquid'].sample(frac=0.1).index)\n",
    "print(f\"The balanced dataset has {len(balanced_data)} samples\")\n",
    "prop_classes(balanced_data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15e7d08",
   "metadata": {},
   "source": [
    "Data is now more balanced, it will be now split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7661e5cd",
   "metadata": {},
   "source": [
    "### Train, validation and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ff9039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full data, used to plot\n",
    "X = data[['temperature', 'pressure']]\n",
    "y = data['label']\n",
    "# Balanced data, will be splitted, used to train\n",
    "X_balanced = balanced_data[['temperature', 'pressure']]\n",
    "y_balanced = balanced_data['label']\n",
    "\n",
    "# Split train and test (80-20%)\n",
    "# random_state is used for reproducibility\n",
    "X_fulltrain, X_test, y_fulltrain, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=17)\n",
    "# Split the fulltrain (80%) to a sub-train and validation (0.25*0.8 = 20% of total)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_fulltrain, y_fulltrain, test_size=0.25, random_state=73)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92b10be",
   "metadata": {},
   "source": [
    "Plot the balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b605d5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.yscale('log')\n",
    "sns.scatterplot(x=balanced_data['temperature'], y=np.power(10, balanced_data['pressure']), hue=balanced_data['phase'], s=15)\n",
    "plt.legend(loc='right')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c545da78",
   "metadata": {},
   "source": [
    "Check if the propotions are roughly the same between the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf66f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_classes(y_fulltrain)\n",
    "prop_classes(y_test)\n",
    "prop_classes(y_train)\n",
    "prop_classes(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35070cdf",
   "metadata": {},
   "source": [
    "---\n",
    "## Classifiers\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04983b05",
   "metadata": {},
   "source": [
    "In this part, three different classifiers will be compared. The pipeline is the same for all of them. In a first phase, using the train and validation sets, the hyperparameters are optimized in a grid search manner. After that, the train and validation sets are considered as the *fulltrain* set. The optimized model (i.e. with the best hyperparameters) is trained on it, and finally tested on the test set. The two metrics used are the accuracy and the area under the ROC curve (AUC), but the selection of the *best* model is chosen w.r.t. the accuracy first, then the AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359535b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will held the results of each classifier on the test set\n",
    "final_results = pd.DataFrame(columns=['classifier', 'accuracy', 'AUC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdd563e",
   "metadata": {},
   "source": [
    "## Support vector machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73a0291",
   "metadata": {},
   "source": [
    "Hyperparameters optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dbb20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and eval SKLearn model (here SVM and k-NN)\n",
    "def train_eval_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = (predictions == y_test).sum() / len(predictions)\n",
    "    proba_predictions = model.predict_proba(X_test)\n",
    "    # AUC is computed in a 'one vs one' manner, averaged\n",
    "    auc = AUC(y_test, proba_predictions, multi_class='ovo')\n",
    "    return accuracy, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc51568",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the hyperparameters for the grid search\n",
    "kernels = ['rbf', 'poly']\n",
    "c_values = np.logspace(-4, 10, 15)\n",
    "gamma_values = np.logspace(-15, 1, 17)\n",
    "degree_values = range(1, 4)\n",
    "# Contains the hyperparameters and the result metrics\n",
    "scores_svm = pd.DataFrame(columns=['kernel', 'C', 'gamma', 'degree', 'accuracy', 'auc'])\n",
    "\n",
    "# Optimize the hyperparameters using grid search\n",
    "for kernel in kernels:\n",
    "    print(f\"Optimizing {kernel} kernel\")\n",
    "    if kernel == 'rbf':\n",
    "        for c in c_values:\n",
    "            for gamma in gamma_values:\n",
    "                # Set a max_iter to reduce the run time\n",
    "                svm_model = SVC(C=c, gamma=gamma, kernel=kernel, probability=True, max_iter=100000, decision_function_shape='ovo')\n",
    "                accuracy, auc = train_eval_model(svm_model, X_train, y_train, X_val, y_val)\n",
    "                # Aggregate results\n",
    "                new_row = pd.DataFrame({\n",
    "                    'kernel': kernel,\n",
    "                    'C': c,\n",
    "                    'gamma': gamma,\n",
    "                    'accuracy': np.around(accuracy, decimals=6),\n",
    "                    'auc': np.around(auc, decimals=6),\n",
    "                }, index=[0])\n",
    "                scores_svm = pd.concat([scores_svm, new_row], ignore_index=True)\n",
    "                # To show progression\n",
    "                print('.', end='', flush=True)\n",
    "                \n",
    "    elif kernel == 'poly':\n",
    "        for c in c_values:\n",
    "            for gamma in gamma_values:\n",
    "                for degree in degree_values:\n",
    "                    svm_model = SVC(C=c, gamma=gamma, kernel=kernel, degree=degree, probability=True, max_iter=100000, decision_function_shape='ovo')\n",
    "                    accuracy, auc = train_eval_model(svm_model, X_train, y_train, X_val, y_val)\n",
    "                    new_row = pd.DataFrame({\n",
    "                        'kernel': kernel,\n",
    "                        'C': c,\n",
    "                        'gamma': gamma,\n",
    "                        'degree': degree,\n",
    "                        'accuracy': np.around(accuracy, decimals=6),\n",
    "                        'auc': np.around(auc, decimals=6),\n",
    "                    }, index=[0])\n",
    "                    scores_svm = pd.concat([scores_svm, new_row], ignore_index=True)\n",
    "                    print('.', end='', flush=True)\n",
    "    print()\n",
    "\n",
    "# Sort the scores and print the best hyperparameters\n",
    "sorted_scores_svm = scores_svm.sort_values(by=['accuracy', 'auc'], ascending=False)\n",
    "print()\n",
    "# Best model is the first in the sorted list\n",
    "print(f\"The best accuracy is with \\n {sorted_scores_svm.iloc[0]}\")\n",
    "sorted_scores_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c8ff6e",
   "metadata": {},
   "source": [
    "Take the best hyperparameters, and run the model with the *fulltrain* set as training set. Finally test it on the official test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df991a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns of the metrics, and where there are NaN\n",
    "best_hyperparams_svm = sorted_scores_svm.iloc[0].drop(['accuracy', 'auc']).dropna()\n",
    "# Build the best model, train and evaluate it\n",
    "best_model_svm = SVC(**best_hyperparams_svm, probability=True, decision_function_shape='ovo')\n",
    "acc, auc = train_eval_model(best_model_svm, X_fulltrain, y_fulltrain, X_test, y_test)\n",
    "# Aggregate results\n",
    "new_row = pd.DataFrame({\n",
    "    'classifier': 'SVM',\n",
    "    'accuracy': acc,\n",
    "    'AUC': auc,\n",
    "}, index=[0])\n",
    "final_results = pd.concat([final_results, new_row], ignore_index=True)\n",
    "final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0167a1",
   "metadata": {},
   "source": [
    "Predict results on the full original dataset and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1541b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom color map\n",
    "colors = [\"#3B4CC0\", \"#B40426\", \"#F6BE00\"]\n",
    "color_map = ListedColormap(colors)\n",
    "\n",
    "# Creating meshgrid for the background plot\n",
    "x_arr = np.linspace(200, 575, 300)\n",
    "y_arr = np.linspace(-1, 7, 300)\n",
    "xs, ys = np.meshgrid(x_arr, y_arr)\n",
    "# Input data for predictions\n",
    "input_data = np.c_[xs.ravel(), ys.ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eba20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_points_and_background(xs, ys, predictions, color_map, classifier):\n",
    "    plt.yscale('log')\n",
    "    # Transform pressure feature to initial numbers\n",
    "    plt.pcolormesh(xs, np.power(10, ys), predictions, cmap=color_map, alpha=0.5)\n",
    "    # Plot the data points and add title and labels\n",
    "    plt.scatter(X['temperature'], np.power(10, X['pressure']), c=y, s=20, cmap=color_map, edgecolor='k')\n",
    "    plt.xlabel(\"Temperature [K]\")\n",
    "    plt.ylabel(\"Pressure [Pa]\")\n",
    "    plt.title(f\"Classification with {classifier}\")\n",
    "    # Handle legends\n",
    "    patch0 = mpatches.Patch(color=colors[0], label='Gas')\n",
    "    patch1 = mpatches.Patch(color=colors[1], label='Liquid')\n",
    "    patch2 = mpatches.Patch(color=colors[2], label='Solid')\n",
    "    patch3 = mpatches.Patch(color='k', label='Transition line')\n",
    "    plt.legend(handles=[patch0, patch1, patch2, patch3], loc=\"lower right\")\n",
    "\n",
    "    # Plot theoretical transition lines\n",
    "    # Equations are taken from the paper \"New Equations for the Sublimation Pressure and Melting Pressure of H2O Ice Ih\"\n",
    "    # Triple point pressure and temperature of water\n",
    "    pt = 611.657\n",
    "    tt = 273.16\n",
    "    # Sublimation line\n",
    "    sublimation_fn = lambda T: pt * np.exp((-0.212144006e2 * (T/tt)**0.00333333333\n",
    "              + 0.273203819e2 * (T/tt)**1.20666667\n",
    "              - 0.610598130e1 * (T/tt)**1.70333333) * tt / T)\n",
    "    # Vaporization line\n",
    "    vaporization_fn = lambda T: pt * np.exp(-(42e3/8.3145) * (1/T - 1/tt))\n",
    "    # Compute values of pressure\n",
    "    inf_points = xs[xs <= tt]\n",
    "    sup_points = xs[xs > tt]\n",
    "    sublimation_vals = sublimation_fn(inf_points)\n",
    "    vaporization_vals = vaporization_fn(sup_points)\n",
    "    # Plot the three transition lines\n",
    "    sns.lineplot(x=inf_points, y=sublimation_vals, c='k', linewidth=2)\n",
    "    sns.lineplot(x=[tt, 272.74247492], y=[pt, 1e7], c='k', linewidth=2)\n",
    "    sns.lineplot(x=sup_points, y=vaporization_vals, c='k', linewidth=2)\n",
    "\n",
    "    # Export the figure\n",
    "    plt.savefig(output_path / f\"classifier_{classifier}.pdf\")\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ec1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for the whole dataset\n",
    "predictions = best_model_svm.predict(input_data)\n",
    "predictions = predictions.reshape(xs.shape)\n",
    "plot_points_and_background(xs, ys, predictions, color_map, 'SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff414ec",
   "metadata": {},
   "source": [
    "## $k$-nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e7b0b0",
   "metadata": {},
   "source": [
    "Hyperparameters optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eee757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters for the grid search\n",
    "n_neighbors = range(1, 8)\n",
    "weights = ['uniform', 'distance']\n",
    "algorithms = ['ball_tree', 'kd_tree', 'brute']\n",
    "leaf_sizes = range(1, 51)\n",
    "p_values = [1, 2, 3]\n",
    "# Contains the hyperparameter and the result metrics\n",
    "scores_knn = pd.DataFrame(columns=['n_neighbors', 'weights', 'algorithm', 'leaf_size', 'p', 'accuracy', 'auc'])\n",
    "\n",
    "# Optimize the hyperparameters using grid search\n",
    "for algorithm in algorithms:\n",
    "    print(f\"Optimizing {algorithm}\")\n",
    "    for weight in weights:\n",
    "        for n in n_neighbors:\n",
    "            for p_value in p_values:\n",
    "                if algorithm == 'brute':\n",
    "                    # Create, train and evaluate the model\n",
    "                    knn_model = KNN(n, weights=weight, algorithm=algorithm, p=p_value)\n",
    "                    accuracy, auc = train_eval_model(knn_model, X_train, y_train, X_val, y_val)\n",
    "                    # Aggregate results\n",
    "                    new_row = pd.DataFrame({\n",
    "                        'n_neighbors': n,\n",
    "                        'weights': weight,\n",
    "                        'algorithm': algorithm,\n",
    "                        'p': p_value,\n",
    "                        'accuracy': np.around(accuracy, decimals=6),\n",
    "                        'auc': np.around(auc, decimals=6),\n",
    "                    }, index=[0])\n",
    "                    scores_knn = pd.concat([scores_knn, new_row], ignore_index=True)\n",
    "                    print('.', end='', flush=True)\n",
    "                    \n",
    "                else:\n",
    "                    for leaf_size in leaf_sizes:\n",
    "                        knn_model = KNN(n, weights=weight, algorithm=algorithm, leaf_size=leaf_size, p=p_value)\n",
    "                        accuracy, auc = train_eval_model(knn_model, X_train, y_train, X_val, y_val)\n",
    "                        new_row = pd.DataFrame({\n",
    "                            'n_neighbors': n,\n",
    "                            'weights': weight,\n",
    "                            'algorithm': algorithm,\n",
    "                            'leaf_size': leaf_size,\n",
    "                            'p': p_value,\n",
    "                            'accuracy': np.around(accuracy, decimals=6),\n",
    "                            'auc': np.around(auc, decimals=6),\n",
    "                        }, index=[0])\n",
    "                        scores_knn = pd.concat([scores_knn, new_row], ignore_index=True)\n",
    "                        print('.', end='', flush=True)  \n",
    "    print()\n",
    "    \n",
    "# Sort the scores and print the best hyperparameters\n",
    "sorted_scores_knn = scores_knn.sort_values(by=['accuracy', 'auc'], ascending=False)\n",
    "print()\n",
    "# Best model is the first in the sorted list\n",
    "print(f\"The best accuracy is with \\n {sorted_scores_knn.iloc[0]}\")\n",
    "sorted_scores_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15be798",
   "metadata": {},
   "source": [
    "Take the best hyperparameters, and run the model with the *fulltrain* set as training set. Finally test it on the official test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c14b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns of the metrics, and where there are NaN\n",
    "best_hyperparams_knn = sorted_scores_knn.iloc[0].drop(['accuracy', 'auc']).dropna()\n",
    "# Build, train and evaluate the best model\n",
    "best_model_knn = KNN(**best_hyperparams_knn)\n",
    "acc, auc = train_eval_model(best_model_knn, X_fulltrain, y_fulltrain, X_test, y_test)\n",
    "# Aggregate results\n",
    "new_row = pd.DataFrame({\n",
    "    'classifier': 'k-NN',\n",
    "    'accuracy': acc,\n",
    "    'AUC': auc,\n",
    "}, index=[0])\n",
    "final_results = pd.concat([final_results, new_row], ignore_index=True)\n",
    "final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6d002a",
   "metadata": {},
   "source": [
    "Predict results on the full original dataset and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75268009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for the whole dataset\n",
    "predictions = best_model_knn.predict(input_data)\n",
    "predictions = predictions.reshape(xs.shape)\n",
    "plot_points_and_background(xs, ys, predictions, color_map, 'k-NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3d9a01",
   "metadata": {},
   "source": [
    "## Neural network (multilayer perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01541bb9",
   "metadata": {},
   "source": [
    "Handling the data. The data and labels will be stored in a PyTorch dataset. This dataset will then be wrapped in a dataloader, that gives batches when training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096f288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Dataset subclass needs to implement __len__() and __getitem__()\n",
    "class WaterDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.features = X\n",
    "        self.labels = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # dtype is float32 so that it will be the same data type as the weights in the MLP\n",
    "        item_features = tensor(self.features.iloc[idx], dtype=torch.float32)\n",
    "        # dtype long is int64\n",
    "        item_label = tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "        return item_features, item_label\n",
    "    \n",
    "# Create all three datasets\n",
    "train_set = WaterDataset(X_train, y_train)\n",
    "val_set = WaterDataset(X_val, y_val)\n",
    "test_set = WaterDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb60164",
   "metadata": {},
   "source": [
    "Create the multilayer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e1c60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_hidden_layers=2, n_units=20):\n",
    "        super(MLP, self).__init__()\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_units = n_units\n",
    "        assert self.n_hidden_layers >= 0\n",
    "        assert self.n_units >= 1\n",
    "        # If a GPU is available, use it for training\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # Input has size 2 (temperature and pressure)\n",
    "        self.first_layer = nn.Linear(2, n_units)\n",
    "        # Create the number of hidden layers we want, with the number of units (neurons) we specified\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(n_units, n_units) for i in range(n_hidden_layers)])\n",
    "        # Output has size 3 (gas, liquid or solid)\n",
    "        self.output_layer = nn.Linear(n_units, 3)\n",
    "        # Activation function between layers\n",
    "        self.act_fn = nn.Sigmoid()\n",
    "        # Softmax so that we get probabilities for the 3 classes\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    # Forward pass\n",
    "    def forward(self, x):\n",
    "        # Input layer and activation\n",
    "        x = self.act_fn(self.first_layer(x))\n",
    "        # Hidden layers and activation\n",
    "        if self.n_hidden_layers != 0:\n",
    "            for layer in self.hidden_layers:\n",
    "                x = self.act_fn(layer(x))\n",
    "        # Output layer\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ded392",
   "metadata": {},
   "source": [
    "Main train method for the MLP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9689585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp(model, optimizer, epochs, batch_size, learning_rate, train_dataset, validation_dataset, test_dataset):\n",
    "    # Use GPU if available\n",
    "    model.to(model.device)\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    # Create the dataloader and the optimizer\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    # To store the metrics\n",
    "    metrics = {\n",
    "        'train': pd.DataFrame(columns=['loss', 'acc', 'auc']),\n",
    "        'validation': pd.DataFrame(columns=['loss', 'acc', 'auc']),\n",
    "        'test': pd.DataFrame(columns=['loss', 'acc', 'auc']),\n",
    "    }\n",
    "    # To save the best model (early stopping)\n",
    "    best_acc = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(1, epochs+1)):\n",
    "        for features, labels in train_dataloader:\n",
    "            # Send to same device as the model\n",
    "            features = features.to(model.device)\n",
    "            labels = labels.to(model.device)\n",
    "            outputs = model(features)\n",
    "            # Reset gradients to 0\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            # Compute gradients (backpropagation)\n",
    "            loss.backward()\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Evaluate the model on the train set\n",
    "        loss, acc, auc = eval_mlp(model, train_dataset, loss_fn)\n",
    "        # Aggregate the metrics\n",
    "        new_row = pd.DataFrame({\n",
    "            # Item returns the single element in the loss tensor\n",
    "            'loss': loss.item(),\n",
    "            'acc': acc,\n",
    "            'auc': auc,\n",
    "        }, index=[0])\n",
    "        metrics['train'] = pd.concat([metrics['train'], new_row], ignore_index=True)\n",
    "        \n",
    "        # Evaluate the model on the validation set\n",
    "        loss, acc, auc = eval_mlp(model, validation_dataset, loss_fn)\n",
    "        # Aggregate the metrics\n",
    "        new_row = pd.DataFrame({\n",
    "            'loss': loss.item(),\n",
    "            'acc': acc,\n",
    "            'auc': auc,\n",
    "        }, index=[0])\n",
    "        metrics['validation'] = pd.concat([metrics['validation'], new_row], ignore_index=True)\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            # Create a new reference in memory\n",
    "            model_to_save = copy.deepcopy(model)\n",
    "            torch.save(model_to_save.state_dict(), 'best_model.pth')\n",
    "            \n",
    "    # Load the best model and evaluate it on the test set\n",
    "    best_model = copy.deepcopy(model)\n",
    "    best_model.load_state_dict(torch.load('best_model.pth'))\n",
    "    loss, acc, auc = eval_mlp(best_model, test_dataset, loss_fn)\n",
    "    new_row = pd.DataFrame({\n",
    "        'loss': loss.item(),\n",
    "        'acc': acc,\n",
    "        'auc': auc,\n",
    "    }, index=[0])\n",
    "    metrics['test'] = pd.concat([metrics['test'], new_row], ignore_index=True)\n",
    "        \n",
    "    return metrics, best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77997746",
   "metadata": {},
   "source": [
    "Evaluation method for the MLP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c480fc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable gradient calculation            \n",
    "@torch.no_grad()            \n",
    "def eval_mlp(model, dataset, loss_fn):\n",
    "    model.to(model.device)\n",
    "    # Set model to eval mode\n",
    "    model.eval()\n",
    "    dataloader = DataLoader(dataset, 64, shuffle=True)\n",
    "    # Store all labels and predictions to compute metrics all at the same time\n",
    "    full_labels = torch.tensor([]).to(model.device)\n",
    "    full_predictions = torch.tensor([]).to(model.device)\n",
    "    # To compute the prediction\n",
    "    for features, labels in dataloader:\n",
    "        # Send to same device as the model\n",
    "        features = features.to(model.device)\n",
    "        labels = labels.to(model.device)\n",
    "        predictions = model(features)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        # Concatenate labels and predictions of this batch to the other ones\n",
    "        full_labels = torch.cat((full_labels.long(), labels), 0)\n",
    "        full_predictions = torch.cat((full_predictions, predictions), 0)\n",
    "\n",
    "    # Compute the metrics (loss, accuracy and AUC)\n",
    "    loss = loss_fn(full_predictions, full_labels)\n",
    "    correct_pred = (full_predictions.argmax(1) == full_labels).type(torch.float).sum().item()\n",
    "    accuracy = correct_pred / len(dataset)\n",
    "    full_predictions = model.softmax(full_predictions)\n",
    "    # cpu() sends the tensor to the CPU\n",
    "    auc = AUC(full_labels.cpu(), full_predictions.cpu(), multi_class='ovo')\n",
    "    return loss, accuracy, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0a2624",
   "metadata": {},
   "source": [
    "Bootstrap method to initialize the model. It creates models and evaluates them directly on the validation set. Take the best model as initialization for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcb568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(n_models, n_layers, n_units, dataset, loss_fn, verbose=False):\n",
    "    assert n_models >= 0\n",
    "    # Base model\n",
    "    best_model = MLP(n_layers, n_units)\n",
    "    _loss, acc, auc = eval_mlp(best_model, dataset, loss_fn)\n",
    "    best_acc = acc\n",
    "    best_auc = auc\n",
    "    \n",
    "    # Create new models and compare the accuracy\n",
    "    for i in range(n_models):\n",
    "        model = MLP(n_layers, n_units)\n",
    "        _loss, acc, auc = eval_mlp(model, dataset, loss_fn)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_auc = auc\n",
    "            best_model = copy.deepcopy(model)\n",
    "        if verbose: print('.', end='', flush=True)\n",
    "                \n",
    "    return acc, auc, best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117bf04d",
   "metadata": {},
   "source": [
    "Hyperparameters optimization: first try different number of layers and units, and get the best architecture using the bootstrap method. Using this architecture, then optimize the batch size and the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9397c834",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_layers_list = range(5)\n",
    "n_units_list = range(10, 211, 20)\n",
    "batch_sizes = [4, 8, 16, 32]\n",
    "learning_rates = np.logspace(-6, 0, 7)\n",
    "scores_architecture_mlp = pd.DataFrame(columns=['n_layers', 'n_units', 'accuracy', 'auc'])\n",
    "# Contains the hyperparameter and the result metrics\n",
    "scores_mlp = pd.DataFrame(columns=['batch_size', 'learning_rate', 'accuracy', 'auc'])\n",
    "\n",
    "print()\n",
    "print(\"Optimizing architecture\")\n",
    "# Get the most promising number of layers and neurons\n",
    "for n_layers in n_layers_list:\n",
    "    for n_units in n_units_list:\n",
    "        acc, auc, start_model = bootstrap(500, n_layers, n_units, val_set, nn.CrossEntropyLoss())\n",
    "        new_row = pd.DataFrame({\n",
    "            'n_layers': n_layers,\n",
    "            'n_units': n_units,\n",
    "            'accuracy': acc,\n",
    "            'auc': auc,\n",
    "        }, index=[0])\n",
    "        scores_architecture_mlp = pd.concat([scores_architecture_mlp, new_row], ignore_index=True)\n",
    "        print('.', end='', flush=True)  \n",
    "        \n",
    "sorted_scores_architecture_mlp = scores_architecture_mlp.sort_values(by=['accuracy', 'auc'], ascending=False)\n",
    "# Best model is the first in the sorted list\n",
    "print()\n",
    "print(f\"The best accuracy is with \\n {sorted_scores_architecture_mlp.iloc[0]}\")\n",
    "best_n_layers, best_n_units, _, _ = sorted_scores_architecture_mlp.iloc[0]\n",
    "print(sorted_scores_architecture_mlp)\n",
    "     \n",
    "print()    \n",
    "print(\"Optimizing hyperparameters\")\n",
    "# Initialze a (hopefully) promising model\n",
    "_, _, start_model = bootstrap(5000, best_n_layers, best_n_units, val_set, nn.CrossEntropyLoss(), verbose=True)\n",
    "for batch_size in batch_sizes:\n",
    "    for learning_rate in learning_rates:\n",
    "        # Define all parameters we need for the training\n",
    "        kwargs = {\n",
    "            'model': start_model,\n",
    "            'optimizer': torch.optim.Adam,\n",
    "            'epochs': 100,\n",
    "            'batch_size': batch_size,\n",
    "            'learning_rate': learning_rate,\n",
    "            'train_dataset': train_set,\n",
    "            'validation_dataset': val_set,\n",
    "            # Set the test as the validation set\n",
    "            'test_dataset': val_set,\n",
    "        }\n",
    "        metrics, _trained_model = train_mlp(**kwargs)\n",
    "        # Aggregate stats\n",
    "        new_row = pd.DataFrame({\n",
    "            'batch_size': batch_size,\n",
    "            'learning_rate': learning_rate,\n",
    "            'accuracy': np.around(metrics['test']['acc'], decimals=6),\n",
    "            'auc': np.around(metrics['test']['auc'], decimals=6),\n",
    "        }, index=[0])\n",
    "        scores_mlp = pd.concat([scores_mlp, new_row], ignore_index=True)\n",
    "                \n",
    "# Sort the scores and print the best hyperparameters\n",
    "sorted_scores_mlp = scores_mlp.sort_values(by=['accuracy', 'auc'], ascending=False)\n",
    "# Best model is the first in the sorted list\n",
    "print()\n",
    "print(f\"The best accuracy is with \\n {sorted_scores_mlp.iloc[0]}\")\n",
    "sorted_scores_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4f06b1",
   "metadata": {},
   "source": [
    "Take the best hyperparameters, and train the model. Finally test it on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225645c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bootstrap model\")\n",
    "_, _, start_model = bootstrap(5000, best_n_layers, best_n_units, val_set, nn.CrossEntropyLoss(), verbose=True)\n",
    "# Take the best hyperparameters\n",
    "best_batch_size, best_learning_rate, _, _ = sorted_scores_mlp.iloc[0]\n",
    "kwargs = {\n",
    "    'model': start_model,\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'epochs': 500,\n",
    "    'batch_size': best_batch_size,\n",
    "    'learning_rate': best_learning_rate,\n",
    "    'train_dataset': train_set,\n",
    "    'validation_dataset': val_set,\n",
    "    'test_dataset': test_set,\n",
    "}\n",
    "metrics, best_model_mlp = train_mlp(**kwargs)\n",
    "\n",
    "# Aggregate the results of the test set\n",
    "acc = metrics['test']['acc']\n",
    "auc = metrics['test']['auc']\n",
    "new_row = pd.DataFrame({\n",
    "    'classifier': 'MLP',\n",
    "    'accuracy': acc,\n",
    "    'AUC': auc,\n",
    "}, index=[0])\n",
    "final_results = pd.concat([final_results, new_row], ignore_index=True)\n",
    "final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d232e03",
   "metadata": {},
   "source": [
    "Predict results on the full original dataset and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe23381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for the full dataset\n",
    "input_data = torch.tensor(input_data, dtype=torch.float32).to(best_model_mlp.device)\n",
    "raw_predictions = best_model_mlp(input_data)\n",
    "predictions = best_model_mlp.softmax(raw_predictions).argmax(1)\n",
    "predictions = predictions.cpu().numpy().reshape(xs.shape)\n",
    "plot_points_and_background(xs, ys, predictions, color_map, 'MLP')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
